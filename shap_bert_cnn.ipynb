{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 19:44:26.734603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-01 19:44:26.889331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:26.889356: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-01 19:44:26.920321: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-01 19:44:27.558781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:27.558869: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:27.558878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#import shapX as shap\n",
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "import yaml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "import os\n",
    "import scipy as sp\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import re\n",
    "import unicodedata\n",
    "import yaml\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, BatchEncoding, Trainer, TrainingArguments, AdamW, get_scheduler\n",
    "from src.utils.utils import fetch_import_module\n",
    "from pipelines import utils_pipeline\n",
    "from time import gmtime, strftime\n",
    "from tqdm import tqdm\n",
    "from datasets import concatenate_datasets,Dataset,ClassLabel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import sys\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import subprocess\n",
    "from accelerate import Accelerator\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from transformers import BertModel\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "hashtag_re = re.compile(r'\\B(\\#[a-zA-Z0-9]+\\b)(?!;)')\n",
    "html_symbol_re = re.compile(r'&[^ ]+')\n",
    "lbr_re = re.compile(r'\\|LBR\\|')\n",
    "url_re = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateError(x, dataset_names):\n",
    "    error = 0\n",
    "    for dataset in dataset_names:\n",
    "        if x['Labels'] != x[dataset]:\n",
    "            error += 1\n",
    "    return error\n",
    "\n",
    "def cleanTweets(tweet):\n",
    "    text = twitter_username_re.sub(\"[UNK]\",tweet)\n",
    "    text = lbr_re.sub(' ', text)\n",
    "    text = unicodedata.normalize('NFKC',text)\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('RT ',' ')\n",
    "    text = hashtag_re.sub(\"[UNK]\",text)\n",
    "    text = html_symbol_re.sub(\" \",text)\n",
    "    text = url_re.sub(\"[UNK]\",text)\n",
    "    return text\n",
    "\n",
    "def predict(x):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for record in x:\n",
    "        processed_x = preprocess(record, tokenizer)\n",
    "        input_ids.append(processed_x[\"input_ids\"])\n",
    "        attention_masks.append(processed_x[\"attention_mask\"])\n",
    "\n",
    "    input_ids_tensor = torch.cat(input_ids)\n",
    "    attention_masks_tensor = torch.cat(attention_masks)\n",
    "\n",
    "    input = torch.stack((input_ids_tensor, attention_masks_tensor))\n",
    "    reshaped_input = torch.reshape(input, (-1, 2, 512)).to(device)\n",
    "    \n",
    "    output = torch.sigmoid(model(reshaped_input)).cpu().detach().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "def preprocess(batch, tokenizer):\n",
    "        batch = cleanTweets(batch)\n",
    "        truncation_length = 512\n",
    "        return tokenizer(batch, truncation=True, max_length=truncation_length, padding = \"max_length\" ,return_token_type_ids = False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "        \"germeval2018\", \n",
    "        \"germeval2019\",\n",
    "        \"hasoc2019\",\n",
    "        \"hasoc2020\",\n",
    "        \"ihs_labelled\",\n",
    "        \"covid_2021\",\n",
    "        \"telegram_gold\"\n",
    "    ]\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('deepset/gbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abusive_df_list = []\n",
    "neutral_df_list = []\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    # fetch datasets\n",
    "    label2id = {\"neutral\": 0, \"abusive\":1}\n",
    "    # import dataset pipeline\n",
    "    source_module = fetch_import_module(dataset)\n",
    "    # execute get_data_binary in pipeline\n",
    "    dset_list_of_dicts = source_module.get_data_binary()\n",
    "    # convert list to dataframe\n",
    "    dset_df = pd.DataFrame(dset_list_of_dicts)\n",
    "\n",
    "    #map neutral to 0 and abusive to 1\n",
    "    dset_df[\"label\"] = dset_df[\"label\"].map(label2id)\n",
    "\n",
    "    neutral_df = dset_df[dset_df[\"label\"] == 0][:n]\n",
    "    abusive_df = dset_df[dset_df[\"label\"] == 1][:n]\n",
    "\n",
    "    abusive_df_list.append(abusive_df)\n",
    "    neutral_df_list.append(neutral_df)\n",
    "\n",
    "combined_abusive_df = pd.concat(abusive_df_list)\n",
    "combined_neutral_df = pd.concat(neutral_df_list)\n",
    "\n",
    "combined_dataset = pd.concat([combined_abusive_df, combined_neutral_df])[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHAP_BERT_cnn(nn.Module):\n",
    "    def __init__(self, bottleneck_dim = 768):\n",
    "        super(SHAP_BERT_cnn, self).__init__()\n",
    "        # truncation length = 512\n",
    "        self.feature_extractor_conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding = (1,0))\n",
    "        self.feature_extractor_relu = nn.ReLU()\n",
    "        self.feature_extractor_pool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.feature_extractor_fc = nn.Linear(2210, bottleneck_dim)\n",
    "        self.feature_extractor_dropout = nn.Dropout(0.1)\n",
    "        self.feature_extractor_flat = nn.Flatten()\n",
    "       \n",
    "    def forward(self, bert_output, input_is_bert = True):\n",
    "        if input_is_bert:\n",
    "            x = torch.swapaxes(torch.stack(bert_output[2]), 0, 1)\n",
    "        else:\n",
    "            x = bert_output\n",
    "        x = self.feature_extractor_dropout(x)\n",
    "\n",
    "        x = self.feature_extractor_conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.feature_extractor_relu(x)\n",
    "        x = self.feature_extractor_dropout(x)\n",
    "        x = self.feature_extractor_pool(x)\n",
    "        x = self.feature_extractor_dropout(x)\n",
    "        if x.size(dim=0) == 13 and x.size(dim=1) == 170:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.feature_extractor_flat(x)\n",
    "        x = self.feature_extractor_dropout(x)\n",
    "        x = self.feature_extractor_fc(x)\n",
    "        return x\n",
    "\n",
    "class DANN_task_classifier(nn.Module):\n",
    "    def __init__(self, bottleneck_dim = 768, layer_size = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.class_classifier = nn.Sequential()\n",
    "        self.class_classifier.add_module('c_fc1', nn.Linear(bottleneck_dim, layer_size))\n",
    "        self.class_classifier.add_module('c_relu1', nn.ReLU())\n",
    "        self.class_classifier.add_module('c_drop1', nn.Dropout())\n",
    "        self.class_classifier.add_module('c_fc2', nn.Linear(layer_size, layer_size))\n",
    "        self.class_classifier.add_module('c_relu2', nn.ReLU())\n",
    "        self.class_classifier.add_module('c_fc3', nn.Linear(layer_size, 1))\n",
    "        self.class_classifier.add_module('c_flatten', nn.Flatten(start_dim = 0))\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_extractor_output\n",
    "    ):\n",
    "    \n",
    "        x = self.class_classifier(feature_extractor_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "class labelled_only_model(nn.Module):\n",
    "    def __init__(self, feature_extractor_module, task_classifier_module):\n",
    "        super(labelled_only_model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"deepset/gbert-base\")\n",
    "        self.output_hidden_states = True\n",
    "        self.feature_extractor = feature_extractor_module\n",
    "        self.task_classifier = task_classifier_module\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        bert_output = self.bert(input_ids=input_data[:,0], attention_mask=input_data[:,1], return_dict = False, output_hidden_states=self.output_hidden_states)\n",
    "\n",
    "        feature_extractor_output = self.feature_extractor(bert_output)\n",
    "\n",
    "        class_output = self.task_classifier(feature_extractor_output)\n",
    "\n",
    "        return class_output\n",
    "    \n",
    "    # inference for testing\n",
    "    def inference(self, input_data):\n",
    "        bert_output = self.bert(input_ids=input_data[:,0], attention_mask=input_data[:,1], return_dict = False, output_hidden_states=self.output_hidden_states)\n",
    "        \n",
    "        feature_extractor_output = self.feature_extractor(bert_output)\n",
    "\n",
    "        class_output = self.task_classifier(feature_extractor_output)\n",
    "\n",
    "        return class_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# exp_name = \"single_source_germeval2018_germeval2018_fair_false\"\n",
    "# path_state_dict = Path(f\"./model_state_dicts/{exp_name}\")\n",
    "feature_extractor = SHAP_BERT_cnn(850)\n",
    "task_classifier = DANN_task_classifier(850, 850)\n",
    "model = labelled_only_model(feature_extractor, task_classifier).to(device)\n",
    "# model.load_state_dict(torch.load(path_state_dict))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "#     \"\"\" Custom tokenizers conform to a subset of the transformers API.\n",
    "#     \"\"\"\n",
    "#     pos = 0\n",
    "#     offset_ranges = []\n",
    "#     input_ids = []\n",
    "#     for m in re.finditer(r\"\\W\", s):\n",
    "#         start, end = m.span(0)\n",
    "#         offset_ranges.append((pos, start))\n",
    "#         input_ids.append(s[pos:start])\n",
    "#         pos = end\n",
    "#     if pos != len(s):\n",
    "#         offset_ranges.append((pos, len(s)))\n",
    "#         input_ids.append(s[pos:])\n",
    "#     out = {}\n",
    "#     out[\"input_ids\"] = input_ids\n",
    "#     if return_offsets_mapping:\n",
    "#         out[\"offset_mapping\"] = offset_ranges\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 19:44:42.305300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-10-01 19:44:42.305459: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305686: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305766: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305852: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305990: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-01 19:44:42.305999: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-01 19:44:42.307113: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b567ab1a92ea42c99c4f546d16b5efbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#masker = shap.maskers.Text(custom_tokenizer)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mExplainer(predict, masker\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m shap_values \u001b[39m=\u001b[39m explainer(combined_dataset[:\u001b[39m5\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/explainers/_partition.py:135\u001b[0m, in \u001b[0;36mPartition.__call__\u001b[0;34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, fixed_context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    133\u001b[0m     \u001b[39m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m    136\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, fixed_context\u001b[39m=\u001b[39;49mfixed_context, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    137\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/explainers/_explainer.py:258\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     feature_names \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(args))]\n\u001b[1;32m    257\u001b[0m \u001b[39mfor\u001b[39;00m row_args \u001b[39min\u001b[39;00m show_progress(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39margs), num_rows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m explainer\u001b[39m\u001b[39m\"\u001b[39m, silent):\n\u001b[0;32m--> 258\u001b[0m     row_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_row(\n\u001b[1;32m    259\u001b[0m         \u001b[39m*\u001b[39;49mrow_args, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[1;32m    260\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m     values\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    263\u001b[0m     output_indices\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moutput_indices\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/explainers/_partition.py:183\u001b[0m, in \u001b[0;36mPartition.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(out_shape)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mowen(fm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curr_base_value, f11, max_evals \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39m2\u001b[39;49m, outputs, fixed_context, batch_size, silent)\n\u001b[1;32m    185\u001b[0m \u001b[39m# if False:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m#     if self.multi_output:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m#         return [self.dvalues[:,i] for i in range(self.dvalues.shape[1])], oinds\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39m# drop the interaction terms down onto self.values\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[:] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdvalues\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/explainers/_partition.py:308\u001b[0m, in \u001b[0;36mPartition.owen\u001b[0;34m(self, fm, f00, f11, max_evals, output_indexes, fixed_context, batch_size, silent)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m# run the batch\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch_args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 308\u001b[0m     fout \u001b[39m=\u001b[39m fm(batch_masks)\n\u001b[1;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m output_indexes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m         fout \u001b[39m=\u001b[39m fout[:,output_indexes]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:67\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_masking_call(full_masks, zero_index\u001b[39m=\u001b[39mzero_index, batch_size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_full_masking_call(masks, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/utils/_masked_model.py:140\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m         all_masked_inputs[i]\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39mcopy(v))\n\u001b[1;32m    139\u001b[0m joined_masked_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stack_inputs(\u001b[39m*\u001b[39mall_masked_inputs)\n\u001b[0;32m--> 140\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mjoined_masked_inputs)\n\u001b[1;32m    141\u001b[0m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    142\u001b[0m all_outputs\u001b[39m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/shap/models/_model.py:21\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_model(\u001b[39m*\u001b[39;49margs))\n",
      "\u001b[1;32m/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb Cell 13\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack((input_ids_tensor, attention_masks_tensor))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m reshaped_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(\u001b[39minput\u001b[39m, (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m512\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msigmoid(model(reshaped_input))\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/markus/Masterarbeit/Cross-Dataset-Generalization-of-German-Hate-Speech-Datasets/shap.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "#masker = shap.maskers.Text(custom_tokenizer)\n",
    "\n",
    "explainer = shap.Explainer(predict, masker=tokenizer)\n",
    "\n",
    "shap_values = explainer(combined_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
